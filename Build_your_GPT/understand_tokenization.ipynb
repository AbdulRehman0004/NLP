{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f84f1c-1b13-4a53-af09-6c81a49bbfe4",
   "metadata": {},
   "source": [
    "# Understand Tokenization Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b923e5-bd9b-4c24-9324-5d7bf8999875",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the process of converting text into tokens, a fundamental step in LLM Models.\n",
    "Using this you can deal with any type of data i.e, coding, conversational, Q/A, Mathematical problems etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca97c1-8bf4-4270-8963-97f31824d773",
   "metadata": {},
   "source": [
    "## Importing Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9560f8d8-0e98-485e-9d97-88f3838e1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa88576-c301-4f54-a63c-d6b2172419e7",
   "metadata": {},
   "source": [
    "## 1. Running Simple Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2145f93-25be-4d74-9a82-5fa3e11765e1",
   "metadata": {},
   "source": [
    "This section demonstrates a basic approach to tokenization using Python's built-in libraries and PyTorch. We will implement a basic tokenization function. This function will split the text into individual tokens.\n",
    "\n",
    "i. Sample_text: It has simple text sentence. \n",
    "\n",
    "ii. Code_text: It has python code example\n",
    "\n",
    "These both help you to understand how a text and code convert into tokens.\n",
    "\n",
    "Note: Model only understand numerical values. It is necesary to map text into unique numerical ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9009535b-4245-4033-9fe0-b431d7885cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hi! I am excited to take my first step at LLM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3518217b-5339-4983-89c7-3e8518f5984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_text = \"\"\"\n",
    "def calculate_llm_perplexity(model, text, max_length=1024):\n",
    "    tokens = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    return math.exp(loss.item())\n",
    "\n",
    "# Example usage\n",
    "perplexity = calculate_llm_perplexity(gpt2_model, \"Hello, world!\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb0ce8b-7b50-4b59-9882-a1c0003c2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):   \n",
    "    # Hint: Use regex to split the text into words and punctuation\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    # pass  # Temporary placeholder to avoid syntax errors\n",
    "        # Remove any empty strings that may occur\n",
    "    result = [token for token in result if token.strip() != '']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286235de-79b4-4703-a4d6-95aef3c006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized text:\", tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf3e36-e525-4b4f-a9e0-202ef052d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tokenized code:\", tokenize(code_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c43a7-3e71-487c-92bc-4a3f5376f919",
   "metadata": {},
   "source": [
    "## 2. Creating a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f703890-f0b4-4d75-a1c4-37ea1ec4f12f",
   "metadata": {},
   "source": [
    "In this section we will create a function that takes a list of texts as input and returns a dictionary. In it each key is a unique word (or token) from the texts and its corresponding value is a unique index. The function should also reserve a special token <UNK> with index 0 to represent unknown words that may appear in future texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5e506",
   "metadata": {},
   "source": [
    " Note: We added two special token \n",
    "\n",
    "i. \"<|endoftext|>\" : It will use to separate the two unrealted text source.\n",
    "\n",
    "ii.\"<|unk|>\" : It will deal with the unknow text which is not a part of training data and also not present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a448895f-faf6-496f-89d7-cb225e9c0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(texts):\n",
    "    ## Create a function to build a word-level vocabulary from a list of texts\n",
    "    # Use a set to collect unique tokens, then convert to a dictionary\n",
    "    all_tokens = set()\n",
    "    \n",
    "    for text in texts:\n",
    "        preprocessed = tokenize(text)\n",
    "        all_tokens.update(preprocessed)\n",
    "    \n",
    "    all_tokens = sorted(list(all_tokens))\n",
    "    all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "    vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c4f37-1dc6-48a2-bd9a-064c579587f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = [\n",
    "\"Dr. Ava Chen yawned, rubbing her tired eyes as she stared at the lines of code scrolling across her monitor. For months, she had been immersed in the cutting-edge world of Large Language Models, pushing the boundaries of artificial intelligence.\",\n",
    "\"Her latest project aimed to create an LLM that could understand and generate complex scientific theories. As she fine-tuned the model's parameters, Ava couldn't help but wonder about the ethical implications of her work.\",\n",
    "\"Suddenly, an alert flashed on her screen. The model had produced something unprecedentedâ€”a novel theory in quantum mechanics. Ava's heart raced as she read through the output, her mind struggling to grasp the implications.\",\n",
    "\"Was this a breakthrough or a clever combination of existing knowledge? As dawn broke outside her lab, Ava realized her journey into the depths of LLMs had only just begun, with countless questions still unanswered.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "text = sample_dataset[0]\n",
    "print(\"text: \", text)\n",
    "\n",
    "# Tokenize the text and convert tokens to IDs\n",
    "tokens = text.split()  # Simple tokenization; adjust if needed\n",
    "print(tokens)\n",
    "# token_ids = [self.vocab.get(token, self.vocab.get('<UNK>')) for token in tokens]  # Map tokens to IDs\n",
    "# print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e8bf6-d345-4607-aae9-592f06c5b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(sample_dataset)\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac055e16-0b54-40d9-9b8d-3bb05518c1ef",
   "metadata": {},
   "source": [
    "## 3. Implementing a Custom Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c078f7-0b94-416c-9333-60c7b7639cea",
   "metadata": {},
   "source": [
    "We have a lot of text data, but it's all different lengths. We need to make it work for our model. To do this, we'll create two special helpers:\n",
    "\n",
    "1. A `Dataset` class: This will help us prepare our text data for our model. We'll break down the text into smaller pieces and convert it into a format our model can understand.\n",
    "2. A `DataLoader` class: This will help us feed our prepared data to our model in batches. We'll sort the batches by length, add padding to make them all the same size, and create a mask to ignore the extra padding.\n",
    "\n",
    "By using these two helpers, we'll be able to get our data in order and make it easy for our model to work with. This will make our training process smoother and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f8ab068-e39a-4fa1-9563-35a52a34cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, vocab):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the dataset with texts and vocabulary.\n",
    "\n",
    "        :param texts: A list of text samples.\n",
    "        :param vocab: A dictionary representing the vocabulary, where keys are tokens and values are their corresponding IDs.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.texts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Convert a text sample to token IDs using the vocabulary\n",
    "        # Hint: dictionary.get(keyname, value if a certain key doesn't exist) can be helpful\n",
    "        # Tokens = []\n",
    "    \n",
    "         # Get the text sample at index idx\n",
    "        tokens = []\n",
    "        token_ids =[]\n",
    "\n",
    "        # for i in range(idx):\n",
    "        text = self.texts[idx]\n",
    "            \n",
    "        # Tokenize the text and convert tokens to IDs\n",
    "        # tokens = text.split()  # Simple tokenization; adjust if needed\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab.get('<|unk|>')) for token in tokens]  # Map tokens to IDs\n",
    "\n",
    "\n",
    "        return torch.tensor(token_ids) \n",
    "   \n",
    "\n",
    "        \n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17206567-8921-415a-a495-36524692cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    \"Dr. Ava Chen yawned, wondering if 42 truly was the answer to Life, the Universe, and Everything.\",\n",
    "    \"Aspiring AI researchers gather excitedly at the conference, ready to push the boundaries of language models.\",\n",
    "    \"A traveler checks their phone anxiously, hoping Munich's notoriously unpredictable weather won't spoil their vacation plans.\"\n",
    "]\n",
    "# Create a dataset instance\n",
    "dataset = TextDataset(example, vocab)\n",
    "print(dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91ce2dc-f5cf-43bb-bd8f-a8144e406cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "simple_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91c680",
   "metadata": {},
   "source": [
    "# Error Will appear beacuse each sample has different length -- Let's resolve it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6813df26-8873-4452-900f-95ace96f6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a batch of data\n",
    "for batch in simple_dataloader:\n",
    "    print(\"Batch shape:\", batch.shape)\n",
    "    print(\"Sample batch:\", batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824291b2-3280-4682-beaf-a5866a0067b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attempting to iterate through the dataloader:\")\n",
    "try:\n",
    "    for batch in simple_dataloader:\n",
    "        print(\"Processed batch:\", batch)\n",
    "        break\n",
    "except RuntimeError as e:\n",
    "    print(f\"Caught an error: {e}\")\n",
    "    print(\"\\nThis error occurs because we're trying to batch sequences of different lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03413975-cc74-40ca-9ab6-644b7610caa8",
   "metadata": {},
   "source": [
    "Now, let's implement a custom collate_fn to handle variable-length sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195014b6-4b41-4817-b11d-61daaab23d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences = batch\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "543f5e3a-6c54-4c1a-b641-5f11f695d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad045f95-d5e9-4868-8645-c35831c5ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterating through the dataloader with custom collate_fn:\")\n",
    "for batch in dataloader:\n",
    "    print(\"Processed batch shape:\", batch.shape)\n",
    "    print(\"Sample batch:\")\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cd25c-9415-407f-a010-b39478526306",
   "metadata": {},
   "source": [
    "The TextProcessor now successfully handles variable-length sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f9e7b-ddfb-4ec5-87aa-d28bf401179e",
   "metadata": {},
   "source": [
    "## 4. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8607e-fbaa-44a8-9767-77a718b9c98a",
   "metadata": {},
   "source": [
    "Time to combine tokenization, vocabulary creation and data preparation in batches. That's where our `TextProcessor` will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97cf6f66-3878-43ce-b02c-8977ac07b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "\n",
    "\n",
    "        #  Implement tokenization\n",
    "        result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        result = [token for token in result if token.strip() != '']\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        \n",
    "        #  Build vocabulary from a list of texts\n",
    "        all_tokens = set()\n",
    "        \n",
    "        for text in texts:\n",
    "            preprocessed = self.tokenize(text)\n",
    "            all_tokens.update(preprocessed)\n",
    "        \n",
    "        all_tokens = sorted(list(all_tokens))\n",
    "        all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "        self.vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "        return self.vocab\n",
    "    \n",
    "    def create_dataloader(self, texts, batch_size):\n",
    "        token_ids = []\n",
    "        # Create a DataLoader with TextDataset from a list of text\n",
    "        # dataset = TextDataset(texts, vocab)\n",
    "        for text in texts:\n",
    "            preprocessed = self.tokenize(text)\n",
    "            tokens = [self.vocab.get(token, self.vocab.get('<|unk|>')) for token in preprocessed]  # Map tokens to IDs\n",
    "            tokens = torch.tensor(tokens)\n",
    "            token_ids.append(tokens)\n",
    "            \n",
    "        dataloader = DataLoader(token_ids, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1215abd5-3e57-48ba-a4b3-071b34709563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  7,   2,   5,   6, 114,   1,  88,  55, 105,  47,  26,  93,  95,  27,\n",
      "        100,  66,  73,  35,  92,  20,  55,  70,   2,   8,  71,   1,  93,  52,\n",
      "         28,  56,  58, 100,  42, 113,  73,  13,  12,  14,   1,  82, 100,  30,\n",
      "         73,  25,  59,   2]), tensor([  9,  65,  81,  21, 106,  41,  23,  10,  99,  38, 108,  24,  50,  37,\n",
      "         90, 101,   2,   4,  93,  48, 100,  69,   0,  89,  79,   1,   5,  39,\n",
      "          0,  98,  54,  33, 111,  19, 100,  45,  57,  73,  55, 112,   2]), tensor([ 15,   1,  23,  22,  49,  74,  55,  91,   2,  16,  69,  52,  80,  94,\n",
      "        109,  72, 102,  58,  83,  67,   2,   5,   0,  89,  53,  85,  26,  93,\n",
      "         86, 104, 100,  77,   1,  55,  68,  97, 106,  51, 100,  57,   2]), tensor([ 17, 103,  18,  31,  76,  18,  34,  36,  73,  46,  63,   3,   4,  43,\n",
      "         32,  78,  55,  64,   1,   5,  87,  55,  61,  60, 100,  44,  73,  11,\n",
      "         52,  75,  62,  29,   1, 110,  40,  84,  96, 107,   2])]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = [\n",
    "\"Dr. Ava Chen yawned, rubbing her tired eyes as she stared at the lines of code scrolling across her monitor. For months, she had been immersed in the cutting-edge world of Large Language Models, pushing the boundaries of artificial intelligence.\",\n",
    "\"Her latest project aimed to create an LLM that could understand and generate complex scientific theories. As she fine-tuned the model's parameters, Ava couldn't help but wonder about the ethical implications of her work.\",\n",
    "\"Suddenly, an alert flashed on her screen. The model had produced something unprecedentedâ€”a novel theory in quantum mechanics. Ava's heart raced as she read through the output, her mind struggling to grasp the implications.\",\n",
    "\"Was this a breakthrough or a clever combination of existing knowledge? As dawn broke outside her lab, Ava realized her journey into the depths of LLMs had only just begun, with countless questions still unanswered.\"\n",
    "]\n",
    "\n",
    "# Test the TextProcessor\n",
    "processor = TextProcessor()\n",
    "vocab = processor.build_vocab(sample_dataset)\n",
    "\n",
    "dataloader = processor.create_dataloader(sample_dataset, batch_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "120a7934-9b3a-4193-85e1-c00ff9d0a433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch: tensor([[  7,   2,   5,   6, 114,   1,  88,  55, 105,  47,  26,  93,  95,  27,\n",
      "         100,  66,  73,  35,  92,  20,  55,  70,   2,   8,  71,   1,  93,  52,\n",
      "          28,  56,  58, 100,  42, 113,  73,  13,  12,  14,   1,  82, 100,  30,\n",
      "          73,  25,  59,   2],\n",
      "        [  9,  65,  81,  21, 106,  41,  23,  10,  99,  38, 108,  24,  50,  37,\n",
      "          90, 101,   2,   4,  93,  48, 100,  69,   0,  89,  79,   1,   5,  39,\n",
      "           0,  98,  54,  33, 111,  19, 100,  45,  57,  73,  55, 112,   2,   0,\n",
      "           0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(\"Processed batch:\", batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df659c-84cb-4c39-b0fb-1c3e77f61917",
   "metadata": {},
   "source": [
    "#### Congratulations! You've implemented a basic text processing pipeline. This will be useful for handling input data in your LLM projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1567c1-8a15-4868-a1bf-86bdf1d4c231",
   "metadata": {},
   "source": [
    "## Reviewing Tokenization Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4eb1-9c4c-476c-aa53-e7a627d57561",
   "metadata": {},
   "source": [
    "We'll use `tiktoken`at a later stage for tokenization, so let's see what it does and compare it to another simple tokenization library `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e5db2-4c03-4727-8855-89a294708764",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad552f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63ddffd0-0f5c-496e-b782-c27335494a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abdulrehman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd232433-ce63-4249-a025-2d2b516cc011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Hi', '!', 'I', 'am', 'excited', 'to', 'take', 'my', 'first', 'step', 'at', 'LLM', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hi! I am excited to take my first step at LLM.\"\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"NLTK Tokens:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c795464-0c53-4f8f-ba79-5c76bf276887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens for Code: ['def', 'calculate_llm_perplexity', '(', 'model', ',', 'text', ',', 'max_length=1024', ')', ':', 'tokens', '=', 'tokenizer.encode', '(', 'text', ',', 'max_length=max_length', ',', 'truncation=True', ')', 'input_ids', '=', 'torch.tensor', '(', '[', 'tokens', ']', ')', '.to', '(', 'device', ')', 'with', 'torch.no_grad', '(', ')', ':', 'outputs', '=', 'model', '(', 'input_ids', ',', 'labels=input_ids', ')', 'loss', '=', 'outputs.loss', 'return', 'math.exp', '(', 'loss.item', '(', ')', ')', '#', 'Example', 'usage', 'perplexity', '=', 'calculate_llm_perplexity', '(', 'gpt2_model', ',', '``', 'Hello', ',', 'world', '!', \"''\", ')', 'print', '(', 'f', \"''\", 'Perplexity', ':', '{', 'perplexity', ':', '.2f', '}', \"''\", ')']\n"
     ]
    }
   ],
   "source": [
    "nltk_code_tokens = word_tokenize(code_text)\n",
    "print(\"NLTK Tokens for Code:\",nltk_code_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ceb9e-0ba1-47cd-a438-5a043d68ab30",
   "metadata": {},
   "source": [
    "### Using Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad0a6973-2712-4577-91d6-a2a64fe40342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2422a0e7-b5de-4f53-a202-149fcc4cffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken Tokens: [13347, 0, 358, 1097, 12304, 311, 1935, 856, 1176, 3094, 520, 445, 11237, 13]\n",
      "Decoded Tiktoken Tokens: Hi! I am excited to take my first step at LLM.\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_tokens = enc.encode(sample_text)\n",
    "print(\"Tiktoken Tokens:\", tiktoken_tokens)\n",
    "print(\"Decoded Tiktoken Tokens:\", enc.decode(tiktoken_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b84397a-cee5-4dd5-99fa-d48bb261cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK token count: 13\n",
      "Tiktoken token count: 14\n"
     ]
    }
   ],
   "source": [
    "print(f\"NLTK token count: {len(nltk_tokens)}\")\n",
    "print(f\"Tiktoken token count: {len(tiktoken_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "056a7701-7cf8-4737-9b04-cb09c080b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiktoken Tokens (decoded for readability):\n",
      "[b'\\n', b'def', b' calculate', b'_ll', b'm', b'_per', b'plex', b'ity', b'(model', b',', b' text', b',', b' max', b'_length', b'=', b'102', b'4', b'):\\n', b'   ', b' tokens', b' =', b' tokenizer', b'.encode', b'(text', b',', b' max', b'_length', b'=max', b'_length', b',', b' trunc', b'ation', b'=True', b')\\n', b'   ', b' input', b'_ids', b' =', b' torch', b'.tensor', b'([', b'tokens', b']).', b'to', b'(device', b')\\n', b'   ', b' with', b' torch', b'.no', b'_grad', b'():\\n', b'       ', b' outputs', b' =', b' model', b'(input', b'_ids', b',', b' labels', b'=input', b'_ids', b')\\n', b'   ', b' loss', b' =', b' outputs', b'.loss', b'\\n', b'   ', b' return', b' math', b'.exp', b'(loss', b'.item', b'())\\n\\n', b'#', b' Example', b' usage', b'\\n', b'per', b'plex', b'ity', b' =', b' calculate', b'_ll', b'm', b'_per', b'plex', b'ity', b'(g', b'pt', b'2', b'_model', b',', b' \"', b'Hello', b',', b' world', b'!\")\\n', b'print', b'(f', b'\"', b'Per', b'plex', b'ity', b':', b' {', b'per', b'plex', b'ity', b':.', b'2', b'f', b'}\")\\n']\n",
      "Tiktoken token count: 115\n"
     ]
    }
   ],
   "source": [
    "tiktoken_code_tokens = enc.encode(code_text)\n",
    "print(\"\\nTiktoken Tokens (decoded for readability):\")\n",
    "print(enc.decode_tokens_bytes(tiktoken_code_tokens))\n",
    "print(f\"Tiktoken token count: {len(tiktoken_code_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2cf12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
